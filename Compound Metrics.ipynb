{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "import json\n",
    "\n",
    "from nltk import meteor_score\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import euclidean\n",
    "import pulp\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch import nn\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMT-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_obj(data_path, name):\n",
    "    with open(data_path + name + '.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data\n",
    "    \n",
    "def get_lang_translation(collections, lang):\n",
    "    src = [sample[0] for sample in collections[lang]]\n",
    "    ref = [sample[1] for sample in collections[lang]]\n",
    "    MT = [sample[2] for sample in collections[lang]]\n",
    "    score = [float(sample[3]) for sample in collections[lang]]\n",
    "    return src, ref, MT, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_processing_wmt17():\n",
    "    data_path = sys.path[0] +\"/wmt17-processed-data/data/\"\n",
    "    lang = [\"csen\",\"deen\",\"enru\", \"enzh\", \"fien\",\"lven\",\"ruen\", \"zhen\",\"tren\"]\n",
    "    nums = 0\n",
    "    collections = {}\n",
    "\n",
    "    for l in lang:    \n",
    "        num_sens_lang = 0\n",
    "        score_lang = []    \n",
    "        data = load_obj(data_path, l)\n",
    "        \n",
    "        for i in data.values():\n",
    "            if len(i[3]) > 0: # With human scores\n",
    "\n",
    "                for k in i[3]:\n",
    "                    score_lang.append([i[1],i[2],k[0],k[1]]) # A human score, A sample\n",
    "\n",
    "                nums += len(i[3])\n",
    "                num_sens_lang +=  len(i[3])\n",
    "\n",
    "        collections[l] = score_lang     \n",
    "        \n",
    "    return collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-30K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_processing_Multi30K():\n",
    "    collections = {\n",
    "        \"de\": sub_processing(\"de\"),\n",
    "        \"fr\": sub_processing(\"fr\")\n",
    "    }\n",
    "    \n",
    "    return collections\n",
    "\n",
    "def sub_processing(lang):\n",
    "    data_path = sys.path[0] + \"/human_assessment/\"\n",
    "    \n",
    "    with open(f\"{data_path}MMTsourcedict.json\") as json_file:\n",
    "        src_dict = json.load(json_file)\n",
    "    with open(f\"{data_path}MMTgolddict_{lang}.json\") as json_file:\n",
    "        ref_dict = json.load(json_file)\n",
    "    with open(f\"{data_path}MMTtranslationdict_{lang}.json\") as json_file:\n",
    "        mt_dict = json.load(json_file)\n",
    "    \n",
    "    lang_match = []\n",
    "    assert len(src_dict) == len(ref_dict)\n",
    "    for src_id in src_dict:\n",
    "        if src_id in mt_dict:\n",
    "            for mt in mt_dict[src_id]:\n",
    "                assert len(mt) == 2\n",
    "                lang_match.append([src_dict[src_id], ref_dict[src_id], mt[0], mt[1]])\n",
    "                                   \n",
    "    return lang_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMT-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_processing_wmt20():\n",
    "    data_path = sys.path[0]+\"/wmt20_data/processed_data/\"\n",
    "    lang = [\"neen\",\"ende\",\"eten\", \"enzh\", \"roen\",\"sien\",\"ruen\"]\n",
    "    nums = 0\n",
    "    collections = {}\n",
    "\n",
    "    for l in lang:    \n",
    "        num_sens_lang = 0\n",
    "        score_lang = []\n",
    "        data = load_obj(data_path, l)\n",
    "        \n",
    "        for i in data:\n",
    "            if len(i[1]) > 0: # With human scores\n",
    "\n",
    "                score_lang.append([i[0],[],i[1],float(i[2])]) # A human score, A sample\n",
    "\n",
    "                nums += 1\n",
    "                num_sens_lang +=  1\n",
    "\n",
    "        collections[l] = score_lang     \n",
    "\n",
    "    return collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMD with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Hook Method: Each Layers' output\n",
    "def layer_processing(model):\n",
    "    layers = []\n",
    "\n",
    "    def layer_hook(module, input_, output):\n",
    "        layers.append(output[0])\n",
    "\n",
    "    for i in model.encoder.layer:\n",
    "        i.register_forward_hook(layer_hook)\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_WMD_Model(name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    model = AutoModel.from_pretrained(name, return_dict=True)\n",
    "    # bert_model.embeddings.word_embeddings\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMD Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Weights of each token for WMD\n",
    "## From the aspects of model embedding.\n",
    "def tokens_to_fracdict(tokens):\n",
    "    cntdict = defaultdict(lambda : 0)\n",
    "        \n",
    "    for token in tokens:\n",
    "        cntdict[token] += 1\n",
    "    totalcnt = sum(cntdict.values())\n",
    "    return {token: float(cnt)/totalcnt for token, cnt in cntdict.items()}\n",
    "\n",
    "## From the aspects of model output, considering contextual relationship.\n",
    "## Each tokens means different, even they are the same.\n",
    "def tokens_to_fracdict_contextual(tokens):\n",
    "    return {token: 1/len(tokens) for token in range(len(tokens))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## There are two components can be used as embedding\n",
    "## 1) model embedding \n",
    "## 2) Model output states\n",
    "\n",
    "def embedding_processing(sent1, sent2, tokenizer, model, embed_type=False):\n",
    "    \n",
    "    sent1_tokens = tokenizer.tokenize(sent1)\n",
    "    sent2_tokens = tokenizer.tokenize(sent2)\n",
    "    \n",
    "    if embed_type:\n",
    "        \n",
    "        sent1_buckets = tokens_to_fracdict(sent1_tokens)\n",
    "        sent2_buckets = tokens_to_fracdict(sent2_tokens) \n",
    "        \n",
    "        sent1_embedding = model.embeddings.word_embeddings(torch.tensor(tokenizer.convert_tokens_to_ids(list(sent1_buckets.keys()))))\n",
    "        sent2_embedding = model.embeddings.word_embeddings(torch.tensor(tokenizer.convert_tokens_to_ids(list(sent2_buckets.keys()))))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        sent1_buckets = tokens_to_fracdict_contextual(sent1_tokens)\n",
    "        sent2_buckets = tokens_to_fracdict_contextual(sent2_tokens) \n",
    "        \n",
    "        sent1_id = tokenizer(sent1,return_tensors=\"pt\")\n",
    "        sent2_id = tokenizer(sent2,return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "#         [-8:-7] indicates Roberta-Large layer 17\n",
    "#         [-4,-3] indicates XLM Roberta-Base layer 9\n",
    "        model(sent1_id['input_ids'])\n",
    "        sent1_embedding = torch.mean(torch.stack(layers[-4:-3]).squeeze(1).permute(1,0,2), dim=1)\n",
    "        \n",
    "        model(sent2_id['input_ids'])\n",
    "        sent2_embedding = torch.mean(torch.stack(layers[-4:-3]).squeeze(1).permute(1,0,2), dim=1)\n",
    "    \n",
    "    layers.clear()\n",
    "    \n",
    "    if sent1_embedding.size()[0] - 2 == len(sent1_tokens):\n",
    "        sent1_embedding = sent1_embedding[1:-1,:] # Remove bos and eos tokens\n",
    "\n",
    "    if sent2_embedding.size()[0] - 2 == len(sent2_tokens):\n",
    "        sent2_embedding = sent2_embedding[1:-1,:] # Remove bos and eos tokens  \n",
    "    \n",
    "    \n",
    "    assert len(sent1_buckets) + len(sent2_buckets) == (sent1_embedding.size()[0] + sent2_embedding.size()[0])\n",
    "    \n",
    "    return sent1_buckets, sent2_buckets, sent1_embedding, sent2_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word_mover_distance_probspec(sent1_buckets, sent2_buckets, sent1_embedding, sent2_embedding, lpFile=None,):\n",
    "\n",
    "    # Updated buckets with labeled name\n",
    "    first_sent_buckets = {f\"x{idx}\": item[1] for idx, item in enumerate(sent1_buckets.items())}\n",
    "    second_sent_buckets = {f\"y{idx}\": item[1] for idx, item in enumerate(sent2_buckets.items())}\n",
    "\n",
    "    var_names = list(first_sent_buckets.keys()) + list(second_sent_buckets.keys())\n",
    "     \n",
    "    all_embedding = torch.cat([sent1_embedding, sent2_embedding])\n",
    "        \n",
    "    assert len(var_names) == all_embedding.size(0)\n",
    "    \n",
    "    wordvecs = {token: embedding.detach().numpy() for token, embedding in zip(var_names, all_embedding)}\n",
    "    \n",
    "    \n",
    "    T = pulp.LpVariable.dicts('T_matrix', list(product(var_names, var_names)), lowBound=0)\n",
    "\n",
    "    prob = pulp.LpProblem('WMD', sense=pulp.LpMinimize)\n",
    "    \n",
    "    prob += pulp.lpSum([T[token1, token2]*euclidean(wordvecs[token1], wordvecs[token2])\n",
    "                        for token1, token2 in product(var_names, var_names)])\n",
    "    \n",
    "    for token2 in second_sent_buckets:   #constrains\n",
    "        prob += pulp.lpSum([T[token1, token2] for token1 in first_sent_buckets])==second_sent_buckets[token2]\n",
    "        \n",
    "    for token1 in first_sent_buckets:    #constrains\n",
    "        prob += pulp.lpSum([T[token1, token2] for token2 in second_sent_buckets])==first_sent_buckets[token1]\n",
    "\n",
    "    if lpFile!=None:\n",
    "        prob.writeLP(lpFile)\n",
    "\n",
    "    prob.solve()\n",
    "#     prob.solve(pulp.PULP_CBC_CMD(msg=False))\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word_mover_distance(sent1, sent2, tokenizer, model, embed_type, lpFile=None):\n",
    "    \n",
    "    sent1_buckets, sent2_buckets, sent1_embedding, sent2_embedding = embedding_processing(sent1, sent2, tokenizer, model, embed_type)\n",
    "    \n",
    "    prob = word_mover_distance_probspec(sent1_buckets, sent2_buckets, sent1_embedding, sent2_embedding, lpFile=lpFile)\n",
    "    \n",
    "    return pulp.value(prob.objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluent Based WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import meteor_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain, product\n",
    "\n",
    "def order_penalty(    \n",
    "    reference,\n",
    "    hypothesis,\n",
    "    preprocess=str.lower,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet):\n",
    "    \n",
    "    enum_hypothesis, enum_reference = meteor_score._generate_enums(\n",
    "        hypothesis, reference, preprocess=preprocess\n",
    "    )\n",
    "    \n",
    "    translation_length = len(enum_hypothesis)\n",
    "    reference_length = len(enum_reference)\n",
    "    \n",
    "    matches, _, _ = meteor_score._enum_allign_words(enum_hypothesis, enum_reference, stemmer=stemmer)\n",
    "    \n",
    "    matches_count = len(matches)\n",
    "    \n",
    "    try:\n",
    "        chunk_count = float(meteor_score._count_chunks(matches))\n",
    "        frag_frac = chunk_count / matches_count\n",
    "        \n",
    "    except ZeroDivisionError: # No unigrams match\n",
    "        return 0\n",
    "    \n",
    "    return frag_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fluency_based_wmd(wmd, ref, hypo, gamma=0.2):\n",
    "    \n",
    "    frag_penalty = order_penalty(ref, hypo)\n",
    "\n",
    "    return wmd - gamma *(0.5 - frag_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceMoverDistance(sents1, sents2, tokenizer, model, embed_type=False):\n",
    "    \n",
    "    sentence_distance = []\n",
    "    \n",
    "    for sent1, sent2 in zip(sents1, sents2):\n",
    "        \n",
    "        _,_,sent1_embedding, sent2_embedding = embedding_processing(sent1, sent2, tokenizer, model, embed_type)\n",
    "    \n",
    "        smd = euclidean(torch.mean(sent1_embedding, axis = 0).detach().numpy(), torch.mean(sent2_embedding,axis=0).detach().numpy())\n",
    "        \n",
    "        sentence_distance.append(smd)\n",
    "        \n",
    "    sentence_distance = (sentence_distance -np.min(sentence_distance))/ (np.max(sentence_distance)-np.min(sentence_distance))\n",
    "\n",
    "    return sentence_distance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getSentSimilarity(sents1, sents2, model):\n",
    "    embed_sent1 = model.encode(sents1, convert_to_tensor=True)\n",
    "    embed_sent2 = model.encode(sents2, convert_to_tensor=True)\n",
    "    cos_sim = nn.CosineSimilarity(dim=1)(embed_sent1,embed_sent2)\n",
    "    # Normalized\n",
    "    cos_sim = (cos_sim -torch.min(cos_sim))/ (torch.max(cos_sim)-torch.min(cos_sim))\n",
    "    return cos_sim.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "bert_score_metric = load_metric('bertscore', keep_in_memory=True, cache_dir=sys.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_type: bert-base-multilingual-cased, xlm-roberta-base\n",
    "\n",
    "def getBertScore(sents1, sents2, model):\n",
    "    bert_score_metric.add_batch(predictions=sents2, references=sents1)\n",
    "    score = bert_score_metric.compute(model_type=model)\n",
    "    # Normalized Bert Score F1\n",
    "    norm_score = (score[\"f1\"] -torch.min(score[\"f1\"]))/ (torch.max(score[\"f1\"])-torch.min(score[\"f1\"]))\n",
    "    return norm_score.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "args: lists of metrics with same length.\n",
    "corr: a list to store the correlation relation of metrics with target. \n",
    "    -1 represents negatively correlated.\n",
    "     1  represents postively correlated.\n",
    "'''\n",
    "\n",
    "def combine_metrics(*args, **kwargs):\n",
    "    assert len(args) == len(kwargs[\"corr\"])\n",
    "    assert len(args[0]) == len(args[1])\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for i in range(len(args[0])):\n",
    "        value = 0\n",
    "        for sign, metric in zip(kwargs[\"corr\"],args):\n",
    "            assert metric[i] <= 1 and metric[i] >= 0\n",
    "            if sign > 0:\n",
    "                value += np.exp(metric[i])\n",
    "            else:\n",
    "                value += np.exp(1-metric[i])\n",
    "        output.append(value)\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "sents: a collection of language sentences with structure [src, ref, MT, score]\n",
    "tokenizer: wmd tokenizer\n",
    "model: wmd model\n",
    "embed_type: BPE embedding or Model outputs\n",
    "Fluent: Fluent-based WMD or not \n",
    "cross_ling: SRC-MT or REF_MT\n",
    "'''\n",
    "\n",
    "def compute_WMD_WMDo(sents, tokenizer, model, embed_type=False, fluent=False, cross_ling=False):\n",
    "    \n",
    "    wmd = []\n",
    "    wmdo =[]\n",
    "    \n",
    "    for i in range(len(sents)):  # Sent structure: [src, ref, MT, score]\n",
    "        hypothesis = sents[i][2]\n",
    "        \n",
    "        if cross_ling:\n",
    "            reference = sents[i][0]   # src - mt\n",
    "        else:\n",
    "            reference = sents[i][1]   # ref - mt\n",
    "        \n",
    "        wmd_tmp = word_mover_distance(reference, hypothesis, tokenizer, model, embed_type)\n",
    "        wmd.append(wmd_tmp)\n",
    "\n",
    "        if fluent:\n",
    "            wmdo.append(fluency_based_wmd(wmd_tmp, reference, hypothesis))\n",
    "                \n",
    "    # Normalize\n",
    "    wmd = [(val-min(wmd))/(max(wmd)-min(wmd)) for val in wmd]\n",
    "    wmdo = [(val-min(wmdo))/(max(wmdo)-min(wmdo)) for val in wmdo]\n",
    "\n",
    "    return np.array(wmd), np.array(wmdo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Correlation evaluation\n",
    "def evaluation(wmd, score):\n",
    "    pearson = stats.pearsonr(wmd, score)\n",
    "    spearman = stats.spearmanr(wmd, score)\n",
    "    print(\"Spearman Correlation:\", spearman)\n",
    "    print(\"Pearson Correlation:\", pearson)\n",
    "    return pearson, spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Save Metrics\n",
    "def save_metrics(name, metric, score):\n",
    "    filePath = f\"{sys.path[0]}/Metrics/{name}\"\n",
    "    file = open(f\"{filePath}.pkl\", 'wb') \n",
    "    pickle.dump([metric, score], file)\n",
    "    file.close()\n",
    "\n",
    "def load_metrics(name):\n",
    "    filePath = f\"{sys.path[0]}/Metrics/{name}\"\n",
    "    file = open(filePath, 'rb')\n",
    "    data = pickle.load(file)\n",
    "    file.close()\n",
    "    return data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## metric: numpy array\n",
    "## score: numpy array\n",
    "def scatter_diagram(*args, legend, score):\n",
    "    score = np.array(score)\n",
    "    score = (score - np.min(score)) / (np.max(score)-np.min(score))\n",
    "    for metric in args:\n",
    "        metric = np.array(metric)\n",
    "        metric = (metric - np.min(metric)) / (np.max(metric)-np.min(metric))\n",
    "        plt.scatter(score, metric)\n",
    "    plt.xlabel(\"human score\")\n",
    "    plt.ylabel(\"Normalized score\")\n",
    "    plt.legend(legend)\n",
    "    plt.plot([0,1],[0,1], \"r--\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wmd_tokenizer, wmd_model = get_WMD_Model('xlm-roberta-base')\n",
    "# wmd_tokenizer, wmd_model = get_WMD_Model('roberta-large') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bert_score_model = 'roberta-large'\n",
    "bert_score_model = 'xlm-roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cos_sim_model = SentenceTransformer('xlm-r-bert-base-nli-stsb-mean-tokens')\n",
    "# cos_sim_model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layers = layer_processing(wmd_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "wmd_tokenizer, wmd_model: tokenizers and pretrained model used in the wmd\n",
    "bert_score_model: specified model type used in the bert score\n",
    "cos_sim_model: cosine similarity model to compute the embedding of sentences\n",
    "save_path: save the each metric with human score \n",
    "lang: Provides the language to be estimated. Otherwise, the whole \n",
    "fluent: Whether fluent based Wmdo are used\n",
    "cross_ling: \n",
    "    True: Cross-linguistic, evaluate src - MT\n",
    "    False: Mono-linguistic, evaluate ref - MT\n",
    "'''\n",
    "\n",
    "def WMT20_Testing(wmd_tokenizer, wmd_model, bert_score_model, cos_sim_model, save_path, langs=None, fluent=False, cross_ling=False):\n",
    "    if not langs:\n",
    "        langs = [\"neen\",\"ende\",\"eten\", \"enzh\", \"roen\",\"sien\",\"ruen\"]\n",
    "    wmt20_collections = data_processing_wmt20()\n",
    "    testing(collections = wmt20_collections, \n",
    "            langs = langs, \n",
    "            wmd_tokenizer = wmd_tokenizer, \n",
    "            wmd_model = wmd_model, \n",
    "            bert_score_model = bert_score_model, \n",
    "            cos_sim_model = cos_sim_model, \n",
    "            save_path = save_path, \n",
    "            fluent=fluent, \n",
    "            cross_ling=cross_ling)\n",
    "    \n",
    "def WMT17_Testing(wmd_tokenizer, wmd_model, bert_score_model, cos_sim_model, save_path, langs=None, fluent=False, cross_ling=False):\n",
    "    if not langs:\n",
    "        if not cross_ling:\n",
    "            langs = [\"csen\",\"deen\",\"enru\", \"enzh\", \"fien\",\"lven\",\"ruen\", \"zhen\", \"tren\"]\n",
    "        else:\n",
    "            langs = [\"csen\",\"deen\",\"fien\",\"lven\",\"ruen\", \"zhen\", \"tren\"]\n",
    "            \n",
    "    wmt17_collections = data_processing_wmt17()\n",
    "    testing(\n",
    "            collections = wmt17_collections, \n",
    "            langs = langs, \n",
    "            wmd_tokenizer = wmd_tokenizer, \n",
    "            wmd_model = wmd_model, \n",
    "            bert_score_model = bert_score_model, \n",
    "            cos_sim_model = cos_sim_model, \n",
    "            save_path = save_path, \n",
    "            fluent=fluent, \n",
    "            cross_ling=cross_ling)\n",
    "\n",
    "    \n",
    "def Multi_30K_Testing(wmd_tokenizer, wmd_model, bert_score_model, cos_sim_model, save_path, langs=None, fluent=False, cross_ling=False):\n",
    "    if not langs:\n",
    "        langs = [\"fr\",\"de\"]\n",
    "    Multi30K_collections = data_processing_Multi30K()\n",
    "    testing(collections = Multi30K_collections, \n",
    "            langs = langs, \n",
    "            wmd_tokenizer = wmd_tokenizer, \n",
    "            wmd_model = wmd_model, \n",
    "            bert_score_model = bert_score_model, \n",
    "            cos_sim_model = cos_sim_model, \n",
    "            save_path = save_path, \n",
    "            fluent=fluent, \n",
    "            cross_ling=cross_ling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "def testing(**kwargs):\n",
    "    \n",
    "    save_path = kwargs['save_path']\n",
    "    \n",
    "    for lang in kwargs[\"langs\"]:\n",
    "        \n",
    "        print(f\"Processing {lang} data:\")\n",
    "        \n",
    "        src, ref, hypothesis, score = get_lang_translation(kwargs[\"collections\"], lang)\n",
    "        \n",
    "        if kwargs[\"cross_ling\"]:             \n",
    "            reference = src\n",
    "        else:\n",
    "            reference = ref\n",
    "            \n",
    "        print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        print(\"One Metric\")\n",
    "        print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        \n",
    "        wmd, wmdo = compute_WMD_WMDo(kwargs[\"collections\"][lang], kwargs[\"wmd_tokenizer\"], kwargs[\"wmd_model\"], embed_type=False, fluent=kwargs[\"fluent\"], cross_ling=kwargs[\"cross_ling\"])\n",
    "        print(f\"Average WMD: \\n{sum(wmd)/len(wmd)}\")\n",
    "        _,_ = evaluation(wmd, score)\n",
    "        #save_metrics(f\"{save_path}/{lang}_wmd\", wmd, score)\n",
    "        print(\"---------------------------------------------\")\n",
    "\n",
    "        smd = getSentenceMoverDistance(hypothesis, reference, kwargs[\"wmd_tokenizer\"], kwargs[\"wmd_model\"], embed_type=False)\n",
    "        print(f\"Average SMD: \\n{sum(smd)/len(smd)}\")\n",
    "        _,_ = evaluation(smd, score)\n",
    "        #save_metrics(f\"{save_path}/{lang}_smd\", smd, score)\n",
    "        print(\"---------------------------------------------\")\n",
    "        \n",
    "        similarity = getSentSimilarity(hypothesis, reference, kwargs[\"cos_sim_model\"])\n",
    "        print(f\"Average Cosine similarity: \\n{sum(similarity)/len(similarity)}\")\n",
    "        _,_ = evaluation(similarity, score)\n",
    "        #save_metrics(f\"{save_path}/{lang}_cs\", similarity, score)\n",
    "        print(\"---------------------------------------------\")\n",
    "        \n",
    "        bert_score = getBertScore(hypothesis, reference, kwargs[\"bert_score_model\"])\n",
    "        print(f\"Average Bert Score: \\n{sum(bert_score)/len(bert_score)}\")\n",
    "        _,_ = evaluation(bert_score, score)\n",
    "        #save_metrics(f\"{save_path}/{lang}_bs\", bert_score, score)\n",
    "        print(\"---------------------------------------------\\n\")\n",
    "        \n",
    "        \n",
    "        print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        print(\"Two metrics\")\n",
    "        print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        compound_metric = combine_metrics(similarity, wmd, corr=[1, -1])\n",
    "        print(f\"Average similarity + wmd: \\n{sum(compound_metric)/len(compound_metric)}\")\n",
    "        _,_ = evaluation(compound_metric, score)\n",
    "        # save_metrics(f\"{save_path}/{lang}_compound\", compound_metric, score)\n",
    "        print(\"---------------------------------------------\")\n",
    "        compound_metric = combine_metrics(bert_score, wmd, corr=[1, -1])\n",
    "        print(f\"Average bert score +  wmd: \\n{sum(compound_metric)/len(compound_metric)}\")\n",
    "        _,_ = evaluation(compound_metric, score)\n",
    "        # save_metrics(f\"{save_path}/{lang}_compound\", compound_metric, score)\n",
    "        print(\"---------------------------------------------\")\n",
    "        compound_metric = combine_metrics(similarity, bert_score, corr=[1, 1])\n",
    "        print(f\"Average similarity + bert score: \\n{sum(compound_metric)/len(compound_metric)}\")\n",
    "        _,_ = evaluation(compound_metric, score)\n",
    "        # save_metrics(f\"{save_path}/{lang}_compound\", compound_metric, score)\n",
    "        print(\"---------------------------------------------\")\n",
    "        compound_metric = combine_metrics(smd, wmd, corr=[-1, -1])\n",
    "        print(f\"Average smd + wmd: \\n{sum(compound_metric)/len(compound_metric)}\")\n",
    "        _,_ = evaluation(compound_metric, score)\n",
    "        # save_metrics(f\"{save_path}/{lang}_compound\", compound_metric, score)\n",
    "        print(\"---------------------------------------------\")\n",
    "        compound_metric = combine_metrics(smd, bert_score, corr=[-1, 1])\n",
    "        print(f\"Average smd + bert score: \\n{sum(compound_metric)/len(compound_metric)}\")\n",
    "        _,_ = evaluation(compound_metric, score)\n",
    "        # save_metrics(f\"{save_path}/{lang}_compound\", compound_metric, score)        \n",
    "        print(\"---------------------------------------------\")\n",
    "\n",
    "        \n",
    "        print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        print(\"Three Metrics\")\n",
    "        print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        compound_metric = combine_metrics(similarity, bert_score, wmd, corr=[1, 1, -1])\n",
    "        print(f\"Average + similarity + bert score - wmd: \\n{sum(compound_metric)/len(compound_metric)}\")\n",
    "        _,_ = evaluation(compound_metric, score)\n",
    "        # save_metrics(f\"{save_path}/{lang}_compound\", compound_metric, score)\n",
    "        print(\"---------------------------------------------\")\n",
    "        compound_metric = combine_metrics(similarity, bert_score, wmd, smd, corr=[1, 1, -1, -1])\n",
    "        print(f\"Average + similarity + bert score - wmd - smd: \\n{sum(compound_metric)/len(compound_metric)}\")\n",
    "        _,_ = evaluation(compound_metric, score)\n",
    "        # save_metrics(f\"{save_path}/{lang}_compound\", compound_metric, score)\n",
    "        print(\"---------------------------------------------\\n\")\n",
    "\n",
    "        \n",
    "        if kwargs[\"fluent\"]:\n",
    "            \n",
    "            print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "            print(\"WMDo Metrics\")\n",
    "            print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        \n",
    "            print(f\"Average WMDo: \\n {sum(wmdo)/len(wmdo)}\")\n",
    "            _,_ = evaluation(wmdo, score)\n",
    "            save_metrics(f\"{save_path}/{lang}_wmdo\", wmdo, score)\n",
    "            print(\"---------------------------------------------\")\n",
    "            compound_metric_o = combine_metrics(similarity, wmdo, corr = [1,-1])\n",
    "            print(f\"Average similarity + wmdo: \\n{sum(compound_metric_o)/len(compound_metric_o)}\")\n",
    "            _,_ = evaluation(compound_metric_o, score)\n",
    "            print(\"---------------------------------------------\")\n",
    "            compound_metric_o = combine_metrics(similarity, wmdo, corr = [-1, 1])\n",
    "            print(f\"Average similarity + wmdo: \\n{sum(compound_metric_o)/len(compound_metric_o)}\")\n",
    "            _,_ = evaluation(compound_metric_o, score)\n",
    "            print(\"---------------------------------------------\")\n",
    "            compound_metric = combine_metrics(similarity, bert_score, wmdo, corr=[1, 1, -1])\n",
    "            print(f\"Average similarity + bert score - wmd: \\n{sum(compound_metric)/len(compound_metric)}\")\n",
    "            _,_ = evaluation(compound_metric, score)\n",
    "            # save_metrics(f\"{save_path}/{lang}_compound\", compound_metric, score)\n",
    "            print(\"---------------------------------------------\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WMT-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "WMT17_Testing(wmd_tokenizer, wmd_model, bert_score_model, cos_sim_model, save_path=\"wmt17/other\", fluent=False, cross_ling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WMT-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WMT20_Testing(wmd_tokenizer, wmd_model, bert_score_model, cos_sim_model, save_path=\"wmt20/src_mt\", fluent=False, cross_ling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Multi-30K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multi_30K_Testing(wmd_tokenizer, wmd_model, bert_score_model, cos_sim_model, save_path=\"multi30K/others/\", fluent=False, cross_ling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
