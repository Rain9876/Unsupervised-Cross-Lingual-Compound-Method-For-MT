{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "import json\n",
    "import re\n",
    "\n",
    "from nltk import meteor_score\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import euclidean\n",
    "import pulp\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from torch import nn\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMT-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(data_path, name):\n",
    "    with open(data_path + name + '.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "#         print(f\"{name} has {len(data)}\")\n",
    "        return data\n",
    "    \n",
    "def get_lang_translation(collections, lang):\n",
    "    src = [sample[0] for sample in collections[lang]]\n",
    "    ref = [sample[1] for sample in collections[lang]]\n",
    "    MT = [sample[2] for sample in collections[lang]]\n",
    "    score = [float(sample[3]) for sample in collections[lang]]\n",
    "    return src, ref, MT, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing_wmt17():\n",
    "    data_path = sys.path[0]+\"/wmt17-processed-data/final_\"\n",
    "    lang = [\"csen\",\"deen\",\"enru\", \"enzh\", \"fien\",\"lven\",\"ruen\", \"zhen\"]\n",
    "    \n",
    "    nums = 0\n",
    "    collections = {}\n",
    "\n",
    "    for l in lang:    \n",
    "\n",
    "        num_sens_lang = 0\n",
    "        score_lang = []\n",
    "        \n",
    "        data = load_obj(data_path, l)\n",
    "        \n",
    "        for i in data.values():\n",
    "            if len(i[3]) > 0: # With human scores\n",
    "\n",
    "                for k in i[3]:\n",
    "                    score_lang.append([i[1],i[2],k[0],k[1]]) # A human score, A sample\n",
    "\n",
    "                nums += len(i[3])\n",
    "                num_sens_lang +=  len(i[3])\n",
    "\n",
    "        collections[l] = score_lang     \n",
    "#         print(f\"{l}: {num_sens_lang}\")\n",
    "#     print(nums)\n",
    "    return collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PASCAL-50S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_french(ref_fr_dict, mt_fr_dict):\n",
    "    tmp = ref_fr_dict.items()\n",
    "    for key, value in tmp:\n",
    "        new_val = re.sub(r\"&\\w+;\\s\",\"'\",value)\n",
    "        ref_fr_dict[key] = new_val\n",
    "\n",
    "    tmp = mt_fr_dict.items()\n",
    "    for key, value in tmp:\n",
    "            new_val = [[re.sub(r\"&\\w+;\\s\",\"'\",item[0]),item[1]]for item in value]\n",
    "            mt_fr_dict[key] = new_val\n",
    "    return ref_fr_dict, mt_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing_pascal():\n",
    "    collections = {\n",
    "        \"de\": sub_processing(\"de\"),\n",
    "        \"fr\": sub_processing(\"fr\")\n",
    "    }\n",
    "#     print(\"De: \" + len(collections[\"de\"]))\n",
    "#     print(\"fr: \" + len(collections[\"fr\"]))\n",
    "    \n",
    "    return collections\n",
    "\n",
    "def sub_processing(lang):\n",
    "    data_path = sys.path[0] + \"/human_assessment/\"\n",
    "    \n",
    "    with open(f\"{data_path}MMTsourcedict.json\") as json_file:\n",
    "        src_dict = json.load(json_file)\n",
    "    with open(f\"{data_path}MMTgolddict_{lang}.json\") as json_file:\n",
    "        ref_dict = json.load(json_file)\n",
    "    with open(f\"{data_path}MMTtranslationdict_{lang}.json\") as json_file:\n",
    "        mt_dict = json.load(json_file)\n",
    "    \n",
    "    if lang == \"fr\": # Additional clean up for French language\n",
    "        ref_dict, mt_dict = clean_up_french(ref_dict, mt_dict)\n",
    "    \n",
    "    lang_match = []\n",
    "    assert len(src_dict) == len(ref_dict)\n",
    "    for src_id in src_dict:\n",
    "        if src_id in mt_dict:\n",
    "            for mt in mt_dict[src_id]:\n",
    "                assert len(mt) == 2\n",
    "                lang_match.append([src_dict[src_id], ref_dict[src_id], mt[0], mt[1]])\n",
    "                                   \n",
    "    return lang_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMT-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing_wmt20():\n",
    "    data_path = sys.path[0]+\"/wmt20_data/processed_data/\"\n",
    "    lang = [\"neen\",\"ende\",\"eten\", \"enzh\", \"roen\",\"sien\",\"ruen\"]\n",
    "    \n",
    "    nums = 0\n",
    "    collections = {}\n",
    "\n",
    "    for l in lang:    \n",
    "\n",
    "        num_sens_lang = 0\n",
    "        score_lang = []\n",
    "        \n",
    "        data = load_obj(data_path, l)\n",
    "        \n",
    "        for i in data:\n",
    "            if len(i[1]) > 0: # With human scores\n",
    "\n",
    "                score_lang.append([i[0],[],i[1],float(i[2])]) # A human score, A sample\n",
    "\n",
    "                nums += 1\n",
    "                num_sens_lang +=  1\n",
    "\n",
    "        collections[l] = score_lang     \n",
    "#         print(f\"{l}: {num_sens_lang}\")\n",
    "#     print(nums)\n",
    "    return collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMD with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hook Method: Each Layers' output\n",
    "def layer_processing(model):\n",
    "    layers = []\n",
    "\n",
    "    def layer_hook(module, input_, output):\n",
    "        layers.append(output[0])\n",
    "\n",
    "    for i in model.encoder.layer:\n",
    "        i.register_forward_hook(layer_hook)\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "# bert_model = AutoModel.from_pretrained('bert-base-multilingual-cased', return_dict=True)\n",
    "# # bert_model.embeddings.word_embeddings\n",
    "# bert_model.eval()\n",
    "# layers = layer_processing(bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-Reborta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# xlm_r_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "# xlm_r_model = AutoModel.from_pretrained(\"xlm-roberta-base\",return_dict=True)\n",
    "# xlm_r_model.eval()\n",
    "# # xlm_r_model.embeddings.word_embeddings\n",
    "# print()\n",
    "# layers = layer_processing(xlm_r_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_WMD_Model(name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    model = AutoModel.from_pretrained(name, return_dict=True)\n",
    "    # bert_model.embeddings.word_embeddings\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMD Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Weights of each token for WMD\n",
    "## From the aspects of model embedding.\n",
    "def tokens_to_fracdict(tokens):\n",
    "    cntdict = defaultdict(lambda : 0)\n",
    "        \n",
    "    for token in tokens:\n",
    "        cntdict[token] += 1\n",
    "    totalcnt = sum(cntdict.values())\n",
    "    return {token: float(cnt)/totalcnt for token, cnt in cntdict.items()}\n",
    "\n",
    "## From the aspects of model output, considering contextual relationship.\n",
    "## Each tokens means different, even they are the same.\n",
    "def tokens_to_fracdict_contextual(tokens):\n",
    "    return {token: 1/len(tokens) for token in range(len(tokens))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are two components can be used as embedding\n",
    "## 1) model embedding \n",
    "## 2) Model output states\n",
    "\n",
    "def embedding_processing(sent1, sent2, tokenizer, model, embed_type):\n",
    "    \n",
    "    sent1_tokens = tokenizer.tokenize(sent1)\n",
    "    sent2_tokens = tokenizer.tokenize(sent2)\n",
    "    \n",
    "    if embed_type == 1:\n",
    "        \n",
    "        sent1_buckets = tokens_to_fracdict(sent1_tokens)\n",
    "        sent2_buckets = tokens_to_fracdict(sent2_tokens) \n",
    "        \n",
    "        sent1_embedding = model.embeddings.word_embeddings(torch.tensor(tokenizer.convert_tokens_to_ids(list(sent1_buckets.keys()))))\n",
    "        sent2_embedding = model.embeddings.word_embeddings(torch.tensor(tokenizer.convert_tokens_to_ids(list(sent2_buckets.keys()))))\n",
    "        \n",
    "    elif embed_type == 2:\n",
    "        \n",
    "#         sent1_buckets = tokens_to_fracdict(sent1_tokens)\n",
    "#         sent2_buckets = tokens_to_fracdict(sent2_tokens) \n",
    "        \n",
    "        sent1_buckets = tokens_to_fracdict_contextual(sent1_tokens)\n",
    "        sent2_buckets = tokens_to_fracdict_contextual(sent2_tokens) \n",
    "        \n",
    "        sent1_id = tokenizer(sent1,return_tensors=\"pt\")\n",
    "        sent2_id = tokenizer(sent2,return_tensors=\"pt\")\n",
    "        \n",
    "#         sent1_embedding = model(sent1_id['input_ids']).last_hidden_state.squeeze(0)\n",
    "#         sent2_embedding = model(sent2_id['input_ids']).last_hidden_state.squeeze(0)\n",
    "        \n",
    "        model(sent1_id['input_ids'])\n",
    "        sent1_embedding = torch.mean(torch.stack(layers[-4:]).squeeze(1).permute(1,0,2), dim=1)\n",
    "        \n",
    "        model(sent2_id['input_ids'])\n",
    "        sent2_embedding = torch.mean(torch.stack(layers[-4:]).squeeze(1).permute(1,0,2), dim=1)\n",
    "    \n",
    "    layers.clear()\n",
    "    \n",
    "    if sent1_embedding.size()[0] - 2 == len(sent1_tokens):\n",
    "        sent1_embedding = sent1_embedding[1:-1,:] # Remove bos and eos tokens\n",
    "\n",
    "    if sent2_embedding.size()[0] - 2 == len(sent2_tokens):\n",
    "        sent2_embedding = sent2_embedding[1:-1,:] # Remove bos and eos tokens  \n",
    "    \n",
    "    \n",
    "    all_embedding = torch.cat([sent1_embedding, sent2_embedding])\n",
    "\n",
    "    assert len(sent1_buckets) + len(sent2_buckets) == all_embedding.size()[0]\n",
    "    \n",
    "    return sent1_buckets, sent2_buckets, all_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_mover_distance_probspec(sent1_buckets, sent2_buckets, all_embedding, lpFile=None,):\n",
    "\n",
    "    # Updated buckets with labeled name\n",
    "    first_sent_buckets = {f\"x{idx}\": item[1] for idx, item in enumerate(sent1_buckets.items())}\n",
    "    second_sent_buckets = {f\"y{idx}\": item[1] for idx, item in enumerate(sent2_buckets.items())}\n",
    "\n",
    "    var_names = list(first_sent_buckets.keys()) + list(second_sent_buckets.keys())\n",
    "    \n",
    "    assert len(var_names) == all_embedding.size(0)\n",
    "    \n",
    "    wordvecs = {token: embedding.detach().numpy() for token, embedding in zip(var_names, all_embedding)}\n",
    "    \n",
    "    \n",
    "    T = pulp.LpVariable.dicts('T_matrix', list(product(var_names, var_names)), lowBound=0)\n",
    "\n",
    "    prob = pulp.LpProblem('WMD', sense=pulp.LpMinimize)\n",
    "    \n",
    "    prob += pulp.lpSum([T[token1, token2]*euclidean(wordvecs[token1], wordvecs[token2])\n",
    "                        for token1, token2 in product(var_names, var_names)])\n",
    "    \n",
    "    for token2 in second_sent_buckets:   #constrains\n",
    "        prob += pulp.lpSum([T[token1, token2] for token1 in first_sent_buckets])==second_sent_buckets[token2]\n",
    "        \n",
    "    for token1 in first_sent_buckets:    #constrains\n",
    "        prob += pulp.lpSum([T[token1, token2] for token2 in second_sent_buckets])==first_sent_buckets[token1]\n",
    "\n",
    "    if lpFile!=None:\n",
    "        prob.writeLP(lpFile)\n",
    "\n",
    "#     prob.solve()\n",
    "    prob.solve(pulp.PULP_CBC_CMD(msg=False))\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_mover_distance(sent1, sent2, tokenizer, model, embed_type, lpFile=None):\n",
    "    \n",
    "    sent1_buckets, sent2_buckets, embeddings = embedding_processing(sent1, sent2, tokenizer, model, embed_type)\n",
    "    \n",
    "    prob = word_mover_distance_probspec(sent1_buckets, sent2_buckets, embeddings, lpFile=lpFile)\n",
    "    \n",
    "    return pulp.value(prob.objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluent Based WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import meteor_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain, product\n",
    "\n",
    "def order_penalty(    \n",
    "    reference,\n",
    "    hypothesis,\n",
    "    preprocess=str.lower,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet):\n",
    "    \n",
    "    enum_hypothesis, enum_reference = meteor_score._generate_enums(\n",
    "        hypothesis, reference, preprocess=preprocess\n",
    "    )\n",
    "    \n",
    "    translation_length = len(enum_hypothesis)\n",
    "    reference_length = len(enum_reference)\n",
    "    \n",
    "    matches, _, _ = meteor_score._enum_allign_words(enum_hypothesis, enum_reference, stemmer=stemmer)\n",
    "    \n",
    "    matches_count = len(matches)\n",
    "    \n",
    "    try:\n",
    "        chunk_count = float(meteor_score._count_chunks(matches))\n",
    "        frag_frac = chunk_count / matches_count\n",
    "        \n",
    "    except ZeroDivisionError: # No unigrams match\n",
    "        return 0\n",
    "    \n",
    "    return frag_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluency_based_wmd(wmd, ref, hypo, gamma=0.2):\n",
    "    \n",
    "    frag_penalty = order_penalty(ref, hypo)\n",
    "\n",
    "    # print(frag_penalty)\n",
    "    \n",
    "    return wmd - gamma *(0.5 - frag_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentSimilarity(sents1, sents2, model):\n",
    "    embed_sent1 = model.encode(sents1, convert_to_tensor=True)\n",
    "    embed_sent2 = model.encode(sents2, convert_to_tensor=True)\n",
    "    cos_sim = nn.CosineSimilarity(dim=1)(embed_sent1,embed_sent2)\n",
    "    # Normalized\n",
    "    cos_sim = (cos_sim -torch.min(cos_sim))/ (torch.max(cos_sim)-torch.min(cos_sim))\n",
    "    return cos_sim.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "bert_score_metric = load_metric('bertscore', keep_in_memory=True, cache_dir=sys.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type: bert-base-multilingual-cased, xlm-roberta-base\n",
    "\n",
    "def getBertScore(sents1, sents2, model):\n",
    "    bert_score_metric.add_batch(predictions=sents2, references=sents1)\n",
    "    score = bert_score_metric.compute(model_type=model)\n",
    "    # Normalized Bert Score F1\n",
    "    norm_score = (score[\"f1\"] -torch.min(score[\"f1\"]))/ (torch.max(score[\"f1\"])-torch.min(score[\"f1\"]))\n",
    "    return norm_score.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Two metrics are used\n",
    "## Metric1 is main metric and metric2 is auxiliary\n",
    "def combine_WMD_Similarity(metric1, metric2):\n",
    "    output = [np.exp(v1) + np.exp(-v2) for v1,v2 in zip(metric1, metric2)]\n",
    "#     output = [-1*v1 + v2 for v1,v2 in zip(wmd, similarity)]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_WMD_WMDo(sents, tokenizer, model, embed_type, fluent=False, cross_ling=False):\n",
    "    \n",
    "    wmd = []\n",
    "    wmdo =[]\n",
    "    \n",
    "    for i in range(len(sents)):  # Sent structure: [src, ref, MT, score]\n",
    "        hypothesis = sents[i][2]\n",
    "        \n",
    "        if cross_ling:\n",
    "            reference = sents[i][0]   # src - mt\n",
    "        else:\n",
    "            reference = sents[i][1]   # ref - mt\n",
    "       \n",
    "        wmd.append(word_mover_distance(reference, hypothesis, tokenizer, model, embed_type))\n",
    "\n",
    "        if fluent:\n",
    "            wmdo.append(fluency_based_wmd(wmd_tmp, reference, hypothesis))\n",
    "                \n",
    "#         if i % 100 == 0:\n",
    "#             print(i)\n",
    "\n",
    "    # Normalize\n",
    "    wmd = [(val-min(wmd))/(max(wmd)-min(wmd)) for val in wmd]\n",
    "    wmdo = [(val-min(wmdo))/(max(wmdo)-min(wmdo)) for val in wmdo]\n",
    "\n",
    "    return np.array(wmd), np.array(wmdo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation evaluation\n",
    "def evaluation(wmd, score):\n",
    "    pearson = stats.pearsonr(wmd, score)\n",
    "    spearman = stats.spearmanr(wmd, score)\n",
    "    print(\"Spearman Correlation:\", spearman)\n",
    "    print(\"Pearson Correlation:\", pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Metrics\n",
    "def save_metrics(name, metric, score):\n",
    "    filePath = f\"{sys.path[0]}/Metrics/{name}\"\n",
    "    file = open(f\"{filePath}.pkl\", 'wb') \n",
    "    pickle.dump([metric, score], file)\n",
    "    file.close()\n",
    "\n",
    "def load_metrics(name):\n",
    "    path = f\"{sys.path[0]}/Metrics/{name}\"\n",
    "    file = open(f\"{filePath}.pkl\", 'rb')\n",
    "    data = pickle.load(file)\n",
    "    file.close()\n",
    "    return data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## metric: numpy array\n",
    "## score: numpy array\n",
    "def scatter_diagram(metric, score):\n",
    "#     metric = metric - np.mean(metric)\n",
    "#     score = score-np.mean(score)\n",
    "    plt.scatter(metric,score)\n",
    "    plt.xlabel(\"human score\")\n",
    "    plt.ylabel(\"Normalized score\")\n",
    "    # plt.legend([\"wmd\", \"wmdo\"])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmd_tokenizer, wmd_model = get_WMD_Model('xlm-roberta-base') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score_model = 'xlm-roberta-base'\n",
    "# bert_score_model = 'bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_model = SentenceTransformer('xlm-r-bert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = layer_processing(wmd_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "wmd_tokenizer, wmd_model: tokenizers and pretrained model used in the wmd\n",
    "bert_score_model: specified model type used in the bert score\n",
    "cos_sim_model: cosine similarity model to compute the embedding of sentences\n",
    "save_path: save the each metric with human score \n",
    "lang: Provides the language to be estimated. Otherwise, the whole \n",
    "fluent: Whether fluent based Wmdo are used\n",
    "cross_ling: \n",
    "    True: Cross-linguistic, evaluate src - MT\n",
    "    False: Mono-linguistic, evaluate ref - MT\n",
    "'''\n",
    "\n",
    "def WMT20_Testing(wmd_tokenizer, wmd_model, bert_score_model, cos_sim_model, save_path, langs=None, fluent=False, cross_ling=False):\n",
    "    if not langs:\n",
    "        langs = [\"neen\",\"ende\",\"eten\", \"enzh\", \"roen\",\"sien\",\"ruen\"]\n",
    "    collections = data_processing_wmt20()\n",
    "    testing(collections, langs, wmd_tokenizer, wmd_model, bert_score_model, cos_sim_model, save_path, fluent=fluent, cross_ling=cross_ling)\n",
    "    \n",
    "def WMT17_Testing(wmd_tokenizer, wmd_model, bert_score_model, cos_sim_model, save_path, langs=None, fluent=False, cross_ling=False):\n",
    "    if not langs:\n",
    "        langs = [\"csen\",\"deen\",\"enru\", \"enzh\", \"fien\",\"lven\",\"ruen\", \"zhen\"]\n",
    "    collections = data_processing_wmt17()\n",
    "    testing(collections, langs, wmd_tokenizer, wmd_model, bert_score_model, cos_sim_model, save_path, fluent=fluent, cross_ling=cross_ling)\n",
    "\n",
    "    \n",
    "def PASCAL_Testing(wmd_tokenizer, wmd_model, bert_score_model, cos_sim_model, save_path, langs=None, fluent=False, cross_ling=False):\n",
    "    if not langs:\n",
    "        langs = [\"fr\",\"de\"]\n",
    "    collections = data_processing_pascal()\n",
    "    testing(collections, langs, wmd_tokenizer, wmd_model, bert_score_model, cos_sim_model, save_path, fluent=fluent, cross_ling=cross_ling)\n",
    "    \n",
    "    \n",
    "def testing(collections, langs, wmd_tokenizer, wmd_model, bert_score_model, cos_sim_model, save_path, fluent=False, cross_ling=False): \n",
    "    \n",
    "    for lang in langs:\n",
    "        \n",
    "        print(f\"Processing {lang} data:\")\n",
    "        \n",
    "        src, ref, hypothesis, score = get_lang_translation(collections, lang)\n",
    "        \n",
    "        if cross_ling:             \n",
    "            reference = src\n",
    "        else:\n",
    "            reference = ref\n",
    "            \n",
    "        wmd, wmdo = compute_WMD_WMDo(collections[lang], wmd_tokenizer, wmd_model, embed_type=2, fluent=fluent, cross_ling=cross_ling)\n",
    "        \n",
    "        print(f\"Average WMD: {sum(wmd)/len(wmd)}\")\n",
    "        evaluation(wmd, score)\n",
    "        save_metrics(f\"{save_path}/{lang}_wmd\", wmd, score)\n",
    "        \n",
    "        similarity = getSentSimilarity(hypothesis, reference, cos_sim_model)\n",
    "        print(f\"Average Cosine similarity: {sum(similarity)/len(similarity)}\")\n",
    "        evaluation(similarity, score)\n",
    "        save_metrics(f\"{save_path}/{lang}_cs\", similarity, score)\n",
    "        \n",
    "        \n",
    "        bert_score = getBertScore(hypothesis, reference, bert_score_model)\n",
    "        print(f\"Average Bert Score: {sum(bert_score)/len(bert_score)}\")\n",
    "        evaluation(bert_score, score)\n",
    "        save_metrics(f\"{save_path}/{lang}_bs\", bert_score, score)\n",
    "        \n",
    "        \n",
    "        compound_metric = combine_WMD_Similarity(similarity, wmd)\n",
    "        print(f\"Average compound metric: {sum(compound_metric)/len(compound_metric)}\")\n",
    "        evaluation(compound_metric, score)\n",
    "        save_metrics(f\"{save_path}/{lang}_cs\", compound_metric, score)\n",
    "        \n",
    "        \n",
    "        if fluent:\n",
    "            print(f\"Average WMDo: {sum(wmdo)/len(wmdo)}\")\n",
    "            evaluation(wmdo, score)\n",
    "            save_metrics(f\"{save_path}/{lang}_wmdo\", wmd, score)\n",
    "            \n",
    "            compound_metric_o = combine_WMD_Similarity(similarity, wmdo)\n",
    "            print(f\"Average compound metric: {sum(compound_metric_o)/len(compound_metric_o)}\")\n",
    "            evaluation(compound_metric_o, score)\n",
    "            save_metrics(f\"{save_path}/{lang}_cso\", compound_metric_o, score)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WMT=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang =\"deen\"\n",
    "# collections = data_processing_wmt17()\n",
    "# src, ref, MT, score = get_lang_translation(collections, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wmd, wmd0 = compute_WMD_WMDo(collections[lang], wmd_tokenizer, wmd_model, embed_type=2, fluent=False, cross_lingual=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation(wmd, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity = getSentSimilarity(ref, MT, cos_sim_model)\n",
    "# evaluation(similarity, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_score = getBertScore(ref, MT, bert_score_model)\n",
    "# evaluation(bert_score, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compound_metric = combine_WMD_Similarity(similarity, wmd)\n",
    "# evaluation(compound_metric, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_metrics(\"deen_src\", compound_metric, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # similarity + wmd\n",
    "# Spearman Correlation: SpearmanrResult(correlation=0.6354514842202942, pvalue=1.1451312453135562e-64)\n",
    "# Pearson Correlation: (0.6474685652507881, 7.521815837611303e-68)\n",
    "# # Bert score\n",
    "# Spearman Correlation: SpearmanrResult(correlation=0.6113722119194076, pvalue=1.0676403977139098e-58)\n",
    "# Pearson Correlation: (0.6199900522366385, 8.944390549431547e-61)\n",
    "# # similarity\n",
    "# Spearman Correlation: SpearmanrResult(correlation=0.5864668846884991, pvalue=4.845536502383926e-53)\n",
    "# Pearson Correlation: (0.5800512616953695, 1.1600227084324783e-51)\n",
    "# # wmd\n",
    "# Spearman Correlation: SpearmanrResult(correlation=-0.5769821833433325, pvalue=5.169696250950864e-51)\n",
    "# Pearson Correlation: (-0.5764824968926663, 6.583882495171967e-51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WMD: 0.44656908581620436\n",
      "Spearman Correlation: SpearmanrResult(correlation=-0.3594431560231835, pvalue=1.6034094853448769e-18)\n",
      "Pearson Correlation: (-0.3731727424348741, 6.045474158745098e-20)\n",
      "Average Cosine similarity: 0.7714485871971452\n",
      "Spearman Correlation: SpearmanrResult(correlation=0.4558142951439806, pvalue=4.433524936294658e-30)\n",
      "Pearson Correlation: (0.4577949452014327, 2.332494014824013e-30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yurunsong/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1324: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Bert Score: 0.6129297337095653\n",
      "Spearman Correlation: SpearmanrResult(correlation=0.4343902335694401, pvalue=3.545459940247522e-27)\n",
      "Pearson Correlation: (0.4456844977556345, 1.1093008814434217e-28)\n",
      "Average compound metric: 2.838093718163001\n",
      "Spearman Correlation: SpearmanrResult(correlation=0.48337695136600756, pvalue=3.947252268042836e-34)\n",
      "Pearson Correlation: (0.4964085345318672, 3.540878161928391e-36)\n"
     ]
    }
   ],
   "source": [
    "lang =\"enzh\"\n",
    "save_path = \"WMT17/src_mt\"\n",
    "collections = data_processing_wmt17()\n",
    "reference, ref, hypothesis, score = get_lang_translation(collections, lang)    \n",
    "wmd, wmdo = compute_WMD_WMDo(collections[lang], wmd_tokenizer, wmd_model, embed_type=2, fluent=False, cross_ling=True)\n",
    "        \n",
    "print(f\"Average WMD: {sum(wmd)/len(wmd)}\")\n",
    "evaluation(wmd, score)\n",
    "save_metrics(f\"{save_path}/{lang}_wmd\", wmd, score)\n",
    "\n",
    "similarity = getSentSimilarity(hypothesis, reference, cos_sim_model)\n",
    "print(f\"Average Cosine similarity: {sum(similarity)/len(similarity)}\")\n",
    "evaluation(similarity, score)\n",
    "save_metrics(f\"{save_path}/{lang}_cs\", similarity, score)\n",
    "\n",
    "bert_score = getBertScore(hypothesis, reference, bert_score_model)\n",
    "print(f\"Average Bert Score: {sum(bert_score)/len(bert_score)}\")\n",
    "evaluation(bert_score, score)\n",
    "save_metrics(f\"{save_path}/{lang}_bs\", bert_score, score)\n",
    "\n",
    "compound_metric = combine_WMD_Similarity(similarity, wmd)\n",
    "print(f\"Average compound metric: {sum(compound_metric)/len(compound_metric)}\")\n",
    "evaluation(compound_metric, score)\n",
    "save_metrics(f\"{save_path}/{lang}_compound\", compound_metric, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation: SpearmanrResult(correlation=-0.38138687577812064, pvalue=7.880118523915315e-21)\n",
      "Pearson Correlation: (-0.39793172755812567, 1.0872668821543204e-22)\n"
     ]
    }
   ],
   "source": [
    "m1 = combine_WMD_Similarity(wmd, bert_score)\n",
    "evaluation(m1, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WMT-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang =\"ne-en\"\n",
    "# collections = data_processing_wmt20()\n",
    "# src, ref, MT, score = get_lang_translation(collections, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wmd, wmd0 = compute_WMD_WMDo(collections[lang], wmd_tokenizer, wmd_model, embed_type=2, fluent=False, cross_lingual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation(wmd, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity = getSentSimilarity(src, MT, cos_sim_model)\n",
    "# evaluation(similarity, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_score = getBertScore(src, MT, bert_score_model)\n",
    "# evaluation(bert_score, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compound_metric = combine_WMD_Similarity(wmd, similarity)\n",
    "# evaluation(compound_metric, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wmd + similarity\n",
    "# Spearman Correlation: SpearmanrResult(correlation=-0.42386348786348793, pvalue=7.177309137106313e-45)\n",
    "# Pearson Correlation: (-0.4010307328248355, 6.296505116193518e-40)\n",
    "# # Bert score    \n",
    "# Spearman Correlation: SpearmanrResult(correlation=0.4161586641586642, pvalue=3.689162607653665e-43)\n",
    "# Pearson Correlation: (0.4065892159098228, 4.2734690689076027e-41)\n",
    "# # similarity    \n",
    "# Spearman Correlation: SpearmanrResult(correlation=0.33880733182375383, pvalue=2.7863502204930564e-28)\n",
    "# Pearson Correlation: (0.31293965321378914, 3.6938392780287396e-24)\n",
    "# # wmd\n",
    "# Spearman Correlation: SpearmanrResult(correlation=-0.3927715887715888, pvalue=3.1227407872395823e-38)\n",
    "# Pearson Correlation: (-0.3748131198452938, 1.0431379070860892e-34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WMD: 0.5447597079218107\n",
      "Spearman Correlation: SpearmanrResult(correlation=-0.26313208713208713, pvalue=2.6718130122671233e-17)\n",
      "Pearson Correlation: (-0.29166047648402227, 4.638414036310341e-21)\n",
      "Average Cosine similarity: 0.8006474618464708\n",
      "Spearman Correlation: SpearmanrResult(correlation=0.4586783186783187, pvalue=3.5416264958295277e-53)\n",
      "Pearson Correlation: (0.44133880200919595, 6.41350465831774e-49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yurunsong/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1324: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Bert Score: 0.5580206314427778\n",
      "Spearman Correlation: SpearmanrResult(correlation=0.2954577434577435, pvalue=1.3556919535309243e-21)\n",
      "Pearson Correlation: (0.3196547139290076, 3.431674756689878e-25)\n",
      "Average compound metric: 2.8483759578133085\n",
      "Spearman Correlation: SpearmanrResult(correlation=0.457913293913294, pvalue=5.524424330449296e-53)\n",
      "Pearson Correlation: (0.4740848525259266, 3.5856506175883255e-57)\n"
     ]
    }
   ],
   "source": [
    "lang =\"ruen\"\n",
    "save_path = \"WMT20/src_mt\"\n",
    "collections = data_processing_wmt20()\n",
    "reference, ref, hypothesis, score = get_lang_translation(collections, lang)\n",
    "\n",
    "wmd, wmdo = compute_WMD_WMDo(collections[lang], wmd_tokenizer, wmd_model, embed_type=2, fluent=False, cross_ling=True)\n",
    "        \n",
    "print(f\"Average WMD: {sum(wmd)/len(wmd)}\")\n",
    "evaluation(wmd, score)\n",
    "# save_metrics(f\"{save_path}/{lang}_wmd\", wmd, score)\n",
    "\n",
    "similarity = getSentSimilarity(hypothesis, reference, cos_sim_model)\n",
    "print(f\"Average Cosine similarity: {sum(similarity)/len(similarity)}\")\n",
    "evaluation(similarity, score)\n",
    "# save_metrics(f\"{save_path}/{lang}_cs\", similarity, score)\n",
    "\n",
    "bert_score = getBertScore(hypothesis, reference, bert_score_model)\n",
    "print(f\"Average Bert Score: {sum(bert_score)/len(bert_score)}\")\n",
    "evaluation(bert_score, score)\n",
    "# save_metrics(f\"{save_path}/{lang}_bs\", bert_score, score)\n",
    "\n",
    "compound_metric = combine_WMD_Similarity(similarity, wmd)\n",
    "print(f\"Average compound metric: {sum(compound_metric)/len(compound_metric)}\")\n",
    "evaluation(compound_metric, score)\n",
    "# save_metrics(f\"{save_path}/{lang}_compound\", compound_metric, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation: SpearmanrResult(correlation=0.409045417045417, pvalue=1.2806445785385669e-41)\n",
      "Pearson Correlation: (0.39535946656418003, 9.299455669166302e-39)\n"
     ]
    }
   ],
   "source": [
    "m1 = combine_WMD_Similarity(similarity, bert_score)\n",
    "evaluation(m1, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PASCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang =\"de\"\n",
    "collections = data_processing_pascal()\n",
    "src, ref, MT, score = get_lang_translation(collections, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmd, wmdo = compute_WMD_WMDo(collections[lang], wmd_tokenizer, wmd_model, embed_type=2, fluent=False, cross_ling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = getSentSimilarity(ref, MT, cos_sim_model)\n",
    "evaluation(similarity, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_metric = combine_WMD_Similarity(similarity, wmd)\n",
    "compound_metric_o = combine_WMD_Similarity(similarity, wmdo)\n",
    "\n",
    "evaluation(compound_metric, score)\n",
    "evaluation(compound_metric_o, score)\n",
    "evaluation(wmd, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_metrics(\"de_src_mt\", compound_metric, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WMD: 0.426280527932965\n",
      "Spearman Correlation: SpearmanrResult(correlation=-0.34767295777287427, pvalue=1.6511001375190912e-72)\n",
      "Pearson Correlation: (-0.3115109157512241, 7.825970965212337e-58)\n",
      "Average Cosine similarity: 0.8005517303662759\n",
      "Spearman Correlation: SpearmanrResult(correlation=0.5769663065131238, pvalue=1.2716484759879025e-223)\n",
      "Pearson Correlation: (0.44880634943075265, 3.6529508344078213e-125)\n",
      "Average Bert Score: 0.6898668720502644\n",
      "Spearman Correlation: SpearmanrResult(correlation=0.3693158281199988, pvalue=2.845619175293309e-82)\n",
      "Pearson Correlation: (0.29082513817044003, 2.6086283768258104e-50)\n",
      "Average compound metric: 2.941454745213255\n",
      "Spearman Correlation: SpearmanrResult(correlation=0.5805957503435236, pvalue=4.439003469240075e-227)\n",
      "Pearson Correlation: (0.4761938931193055, 8.133499336247869e-143)\n"
     ]
    }
   ],
   "source": [
    "lang =\"fr\"\n",
    "save_path = \"PASCAL/src_mt\"\n",
    "collections = data_processing_pascal()\n",
    "reference, ref, hypothesis, score = get_lang_translation(collections, lang)    \n",
    "wmd, wmdo = compute_WMD_WMDo(collections[lang], wmd_tokenizer, wmd_model, embed_type=2, fluent=False, cross_ling=True)\n",
    "        \n",
    "print(f\"Average WMD: {sum(wmd)/len(wmd)}\")\n",
    "evaluation(wmd, score)\n",
    "save_metrics(f\"{save_path}/{lang}_wmd\", wmd, score)\n",
    "\n",
    "similarity = getSentSimilarity(hypothesis, reference, cos_sim_model)\n",
    "print(f\"Average Cosine similarity: {sum(similarity)/len(similarity)}\")\n",
    "evaluation(similarity, score)\n",
    "save_metrics(f\"{save_path}/{lang}_cs\", similarity, score)\n",
    "\n",
    "bert_score = getBertScore(hypothesis, reference, bert_score_model)\n",
    "print(f\"Average Bert Score: {sum(bert_score)/len(bert_score)}\")\n",
    "evaluation(bert_score, score)\n",
    "save_metrics(f\"{save_path}/{lang}_bs\", bert_score, score)\n",
    "\n",
    "compound_metric = combine_WMD_Similarity(similarity, wmd)\n",
    "print(f\"Average compound metric: {sum(compound_metric)/len(compound_metric)}\")\n",
    "evaluation(compound_metric, score)\n",
    "save_metrics(f\"{save_path}/{lang}_compound\", compound_metric, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation: SpearmanrResult(correlation=0.5577853959020562, pvalue=4.7170872087520807e-206)\n",
      "Pearson Correlation: (0.5281844035367238, 4.3126248308364895e-181)\n"
     ]
    }
   ],
   "source": [
    "m1 = combine_WMD_Similarity(similarity, bert_score)\n",
    "evaluation(m1, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compound\n",
    "# Spearman Correlation: SpearmanrResult(correlation=0.5583900565528739, pvalue=7.008387361962618e-285)\n",
    "# Pearson Correlation: (0.5345921289288134, 8.221075657865956e-257)\n",
    "# # WMDo\n",
    "# Spearman Correlation: SpearmanrResult(correlation=0.5593502697055622, pvalue=4.621492592222047e-286)\n",
    "# Pearson Correlation: (0.5361630924475681, 1.3513459056589761e-258)\n",
    "# # WMD\n",
    "# Spearman Correlation: SpearmanrResult(correlation=-0.5089823380124615, pvalue=5.165687764724598e-229)\n",
    "# Pearson Correlation: (-0.49318902941149667, 5.109081581867692e-213)\n",
    "# # Cosine Similarity\n",
    "# Spearman Correlation: SpearmanrResult(correlation=0.533531776812245, pvalue=1.2997631025228642e-255)\n",
    "# Pearson Correlation: (0.48671856252666984, 1.053467840253653e-206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compound\n",
    "# Spearman Correlation: SpearmanrResult(correlation=0.6277348161095964, pvalue=3.092750295851163e-276)\n",
    "# Pearson Correlation: (0.5053510100353606, 1.8336657874693513e-163)\n",
    "# # WMDo\n",
    "# Spearman Correlation: SpearmanrResult(correlation=0.6276570124525135, pvalue=3.789024954110633e-276)\n",
    "# Pearson Correlation: (0.5062459132918665, 3.957301711429879e-164)\n",
    "# # WMD    \n",
    "# Spearman Correlation: SpearmanrResult(correlation=-0.5260006850493311, pvalue=2.3959546961579428e-179)\n",
    "# Pearson Correlation: (-0.43285363745499555, 1.3322431658079695e-115)\n",
    "# # Cosine similarity\n",
    "# Spearman Correlation: SpearmanrResult(correlation=0.6140189852157125, pvalue=4.533173262128837e-261)\n",
    "# Pearson Correlation: (0.4497808438694378, 9.158265216134208e-126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
